{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "provinces = ['BKK','Chiangmai','Khonkaen','Rayong','Saraburi','Surat']\n",
    "province = provinces[4]\n",
    "data = pd.read_csv('./'+province+'/train/'+province.lower()+'_train_format_2.csv')\n",
    "\n",
    "y_train = data.pop('PM2.5')\n",
    "X_train = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1df5d9cd90164cde88e68c9f22083a03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2509a1288bc453395ab4198f088dc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d22038b8d95f46e2997df8325a2cb4a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a2106b50df475daf90e8c82a835c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "i = 1\n",
    "score = []\n",
    "for tr_index, val_index in tqdm_notebook(tscv.split(X_train)):\n",
    "    X_tr, X_val = X_train.iloc[tr_index], X_train.iloc[val_index]\n",
    "    y_tr, y_val = y_train.iloc[tr_index], y_train.iloc[val_index]\n",
    "    for mf in tqdm_notebook(np.linspace(1, 7, 7)):\n",
    "        for ne in np.linspace(20, 200, 10):\n",
    "            for md in np.linspace(20, 80, 6):\n",
    "                for msl in np.linspace(30, 200, 10):\n",
    "                    rfr = RandomForestRegressor(\n",
    "                        max_features=int(mf),\n",
    "                        n_estimators=int(ne),\n",
    "                        max_depth=int(md),\n",
    "                        min_samples_leaf=int(msl),\n",
    "                        n_jobs = -1)\n",
    "                    rfr.fit(X_tr, y_tr)\n",
    "                    y_pred = rfr.predict(X_val)\n",
    "                    score.append([i,\n",
    "                                  mf, \n",
    "                                  ne,\n",
    "                                  md, \n",
    "                                  msl, \n",
    "                                  rfr.score(X_val, y_val),\n",
    "                                  np.sqrt(metrics.mean_squared_error(y_val, y_pred))])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('rf_'+province.lower()+'_score_2.pickle', 'wb') as fp:\n",
    "    pickle.dump(score, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parem: [2, 2.0, 40.0, 20.0, 67.77777777777777, 0.35417922566285087, 17.113411589680062]\n"
     ]
    }
   ],
   "source": [
    "score2 = np.array(score)\n",
    "print('best parem:',score[np.argmin(score2, axis=0)[-1]]) #please note all these params in note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict = {'BKK':[7.0, 70.0, 40.0, 100.0],'Chiangmai':[7.0, 50.0, 40.0, 100.0],'Khonkaen':[1.0, 70.0, 30.0, 90.0],'Rayong':[2.0, 50.0, 30.0, 40.0],'Saraburi':[3.0, 50.0, 20.0, 90.0],'Surat':[2.0, 60.0, 20.0, 30.0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "province = 'BKK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('./'+province+'/train/'+province.lower()+'_train_format_2.csv')\n",
    "# train_data.head()\n",
    "\n",
    "y_train = train_data.pop('PM2.5')\n",
    "X_train = train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(province+'_clean.csv')\n",
    "test_data['date_time'] = pd.to_datetime(test_data['date_time'])\n",
    "test_data['year'] = test_data['date_time'].dt.year\n",
    "test_data['month'] = test_data['date_time'].dt.month\n",
    "test_data['day'] = test_data['date_time'].dt.day\n",
    "test_data['hour'] = test_data['date_time'].dt.hour\n",
    "test_data = test_data[['year','month','day','hour','temp','wind speed','wind dir','PM2.5']]\n",
    "test_data.dropna(inplace=True)\n",
    "# test_data.head()\n",
    "y_test = test_data.pop('PM2.5')\n",
    "X_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 10.575646339818894\n"
     ]
    }
   ],
   "source": [
    "rfr = RandomForestRegressor(\n",
    "                        max_features=int(param_dict[province][0]),\n",
    "                        n_estimators=int(param_dict[province][1]),\n",
    "                        max_depth=int(param_dict[province][2]),\n",
    "                        min_samples_leaf=int(param_dict[province][3]),\n",
    "                        n_jobs = -1)\n",
    "rfr.fit(X_train, y_train)\n",
    "y_pred = rfr.predict(X_test)\n",
    "print('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of BKK: 10.531763558991464\n",
      "RMSE of Chiangmai: 21.129193184149443\n",
      "RMSE of Khonkaen: 12.697137874589506\n",
      "RMSE of Rayong: 9.225255646244463\n",
      "RMSE of Saraburi: 15.02105761057381\n",
      "RMSE of Surat: 7.430652517725827\n",
      "all province RMSE: 13.418868100953738\n"
     ]
    }
   ],
   "source": [
    "SE = []\n",
    "n = 0\n",
    "for province in param_dict:\n",
    "    train_data = pd.read_csv('./'+province+'/train/'+province.lower()+'_train_format_2.csv')\n",
    "    y_train = train_data.pop('PM2.5')\n",
    "    X_train = train_data\n",
    "    test_data = pd.read_csv(province+'_clean.csv')\n",
    "    test_data['date_time'] = pd.to_datetime(test_data['date_time'])\n",
    "    test_data['year'] = test_data['date_time'].dt.year\n",
    "    test_data['month'] = test_data['date_time'].dt.month\n",
    "    test_data['day'] = test_data['date_time'].dt.day\n",
    "    test_data['hour'] = test_data['date_time'].dt.hour\n",
    "    test_data = test_data[['year','month','day','hour','temp','wind speed','wind dir','PM2.5']]\n",
    "    test_data.dropna(inplace=True)\n",
    "    y_test = test_data.pop('PM2.5')\n",
    "    X_test = test_data\n",
    "    rfr = RandomForestRegressor(\n",
    "                        max_features=int(param_dict[province][0]),\n",
    "                        n_estimators=int(param_dict[province][1]),\n",
    "                        max_depth=int(param_dict[province][2]),\n",
    "                        min_samples_leaf=int(param_dict[province][3]),\n",
    "                        n_jobs = -1)\n",
    "    rfr.fit(X_train, y_train)\n",
    "    y_pred = rfr.predict(X_test)\n",
    "    SE.append(np.sum(np.square(np.subtract(np.array(y_test), np.array(y_pred)))))\n",
    "    n += len(y_pred)\n",
    "    print('RMSE of '+province+':',np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print('all province RMSE:',np.sqrt(np.sum(np.array(SE))/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': [1, 23, 45],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_split': [5, 10],\n",
      " 'n_estimators': [20, 65, 110, 155, 200]}\n",
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=10, estimator=RandomForestRegressor(n_jobs=-1), n_jobs=-1,\n",
       "                   param_distributions={'max_depth': [1, 23, 45],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_split': [5, 10],\n",
       "                                        'n_estimators': [20, 65, 110, 155,\n",
       "                                                         200]},\n",
       "                   random_state=42, scoring='neg_mean_squared_error',\n",
       "                   verbose=2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pprint import pprint\n",
    "\n",
    "forest = RandomForestRegressor(n_jobs=-1)\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 20, stop = 200, num = 5)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(1, 45, num = 3)]\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [5, 10]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split}\n",
    "\n",
    "pprint(random_grid)\n",
    "# Use the random grid to search for best hyperparameters\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = forest, param_distributions = random_grid, n_iter = 10, cv = 10, verbose=2, random_state=42, n_jobs = -1, scoring='neg_mean_squared_error')\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.523009241027463 {'n_estimators': 20, 'min_samples_split': 5, 'max_features': 'auto', 'max_depth': 1}\n",
      "20.518289343784335 {'n_estimators': 20, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 1}\n",
      "18.91589175670347 {'n_estimators': 65, 'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': 23}\n",
      "20.59557996481142 {'n_estimators': 20, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 45}\n",
      "21.948674538397665 {'n_estimators': 155, 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 1}\n",
      "18.680708824980332 {'n_estimators': 200, 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 45}\n",
      "18.84261319110746 {'n_estimators': 155, 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 23}\n",
      "20.440627499975733 {'n_estimators': 155, 'min_samples_split': 10, 'max_features': 'auto', 'max_depth': 45}\n",
      "21.934591024764497 {'n_estimators': 110, 'min_samples_split': 5, 'max_features': 'sqrt', 'max_depth': 1}\n",
      "18.685189392489402 {'n_estimators': 110, 'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': 45}\n"
     ]
    }
   ],
   "source": [
    "#now let's how the RMSE changes for each parameter configuration\n",
    "cvres2 = rf_random.cv_results_\n",
    "for mean_score, params in zip(cvres2[\"mean_test_score\"], cvres2[\"params\"]):\n",
    "    print(np.sqrt(-mean_score), params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "723dc80cb05396c6d0019b793cc9629de1a15570232a2a29e62999a170644e05"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
